This project implements a neural-symbolic framework to enhance the reliability and safety of agentic AI systems. By combining neural networks with symbolic reasoning, it aims to prevent hallucinations (incorrect outputs) and mitigate unsafe actions in autonomous AI agents.

## Features

- Neural-symbolic model architecture and training code  
- Data preprocessing and annotation scripts  
- Experiments and evaluation metrics on hallucination detection and safety  
- Detailed documentation covering methodology, usage, and results  
